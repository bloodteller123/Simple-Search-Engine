{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'one': 3, 'six': 2, 'three': 1, 'four': 1, 'five': 1, 'nine': 1})\n",
      "Counter({'one': 5, 'three': 2, 'two': 2, 'ten': 1, 'seven': 1, 'nine': 1})\n",
      "Counter({'right': 1, 'cool': 1, 'fuck': 1, 'computer': 1, 'go': 1, 'what': 1, 'the': 1, 'hee': 1})\n",
      "Counter({'one': 3, 'six': 2, 'three': 1, 'four': 1, 'five': 1, 'nine': 1})\n",
      "Counter({'one': 5, 'three': 2, 'two': 2, 'ten': 1, 'seven': 1, 'nine': 1})\n",
      "Counter({'right': 1, 'cool': 1, 'fuck': 1, 'computer': 1, 'go': 1, 'what': 1, 'the': 1, 'hee': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chenzihan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenzihan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chenzihan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenzihan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import inverted_index_construction_module as iic\n",
    "import dictionary_building_module as db\n",
    "import importlib\n",
    "importlib.reload(iic)\n",
    "importlib.reload(db)\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1059559/split-strings-into-words-with-multiple-word-boundary-delimiters\n",
    "#https://runestone.academy/runestone/books/published/pythonds/BasicDS/InfixPrefixandPostfixExpressions.html\n",
    "#https://stackoverflow.com/questions/1720421/how-do-i-concatenate-two-lists-in-python\n",
    "#https://stackoverflow.com/questions/3697432/how-to-find-list-intersection\n",
    "#Assume there are always spaces between Brackets and words, all connectors are cap\n",
    "# NOT is phrased as AND_NOT as shown in description\n",
    "#https://www.programiz.com/python-programming/methods/string/startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryToPostfix(query):\n",
    "    tokenList = query.split()\n",
    "    postfixList = []\n",
    "    opStack = []\n",
    "    for token in tokenList:\n",
    "            if token == '(':\n",
    "                opStack.append(token)\n",
    "            elif token == ')':\n",
    "                topToken = opStack.pop()\n",
    "                while topToken != '(':\n",
    "                    postfixList.append(topToken)\n",
    "                    topToken = opStack.pop()\n",
    "            elif token == \"OR\" or token == \"AND\" or token == \"AND_NOT\":\n",
    "                opStack.append(token)\n",
    "            else:\n",
    "                postfixList.append(token)\n",
    "\n",
    "    while opStack:\n",
    "        postfixList.append(opStack.pop())\n",
    "    print(postfixList)\n",
    "    return postfixList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPostings(logicalOp, p1, p2):\n",
    "    if logicalOp == 'OR':\n",
    "        return list(set(p1+p2))\n",
    "    elif logicalOp == 'AND':\n",
    "        return list(set(p1) & set(p2))\n",
    "    elif logicalOp == 'AND_NOT': # is it the correct way?\n",
    "        return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildCard_bigram_handle(token,index): # return a new string\n",
    "    # remember to filter our the incorrect word!!!!!!\n",
    "    # are we guaranted that there is only one * in a token?\n",
    "    wildCard = []\n",
    "    \n",
    "    if token[0] == \"*\": #ends with token *abc\n",
    "        temp = token[1:]\n",
    "        for bg in bigrams(temp+\" \"):\n",
    "            t = \"\".join(bg).replace(\" \",\"\")\n",
    "            wildCard.append(buildSecIndex(t,index,True))\n",
    "        # postfilter\n",
    "        wildCard = [x for x in list(set.intersection(*map(set,wildCard))) if x.endswith(temp)]\n",
    "                \n",
    "    elif token[len(token)-1] == '*': # abx*\n",
    "        temp = token[:-1]\n",
    "        for bg in bigrams(\" \"+temp):\n",
    "            t = \"\".join(bg).replace(\" \",\"\")\n",
    "            wildCard.append(buildSecIndex(t,index,False))\n",
    "        wildCard = [x for x in list(set.intersection(*map(set,wildCard))) if x.startswith(temp)]\n",
    "    else: #case where * not in the end or begining \n",
    "        temp = token.split('*')\n",
    "        print(temp)\n",
    "        for bg in bigrams(\" \"+temp[0]): # $abc ???????\n",
    "            t = \"\".join(bg).replace(\" \",\"\")\n",
    "            wildCard.append(buildSecIndex(t,index,False))\n",
    "        for bg in bigrams(temp[1]+\" \"): # zxc$\n",
    "            t = \"\".join(bg).replace(\" \",\"\")\n",
    "            wildCard.append(buildSecIndex(t,index,True))\n",
    "        wildCard = [x for x in list(set.intersection(*map(set,wildCard))) if x.startswith(temp[0]) and x.endswith(temp[1])]\n",
    "\n",
    "    \n",
    "    return listToString(wildCard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(strList):\n",
    "    return \"( \"+\" AND \".join(strList)+\" )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSecIndex(bg,index, isEnd):\n",
    "    secPosting = []\n",
    "    if len(bg)==1 and not isEnd:\n",
    "        for k in index:\n",
    "            if k.startswith(bg): # abc*\n",
    "                secPosting.append(k)\n",
    "    elif len(bg)==1 and isEnd:\n",
    "        for k in index:\n",
    "            if k.endswith(bg): # *abc\n",
    "                secPosting.append(k)\n",
    "    else:\n",
    "        for k in index:\n",
    "            if bg in k:\n",
    "                secPosting.append(k)\n",
    "    return secPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_processWithIndex(query, selectedCollection,index):\n",
    "#     if os.path.exists(iic.indexPath):\n",
    "#         with open(iic.indexPath, 'r') as file:\n",
    "#             f = json.load(file)\n",
    "            \n",
    "            termsPostings = []\n",
    "            #resultList = []\n",
    "            totalPostings = []\n",
    "            # need to apply stemming etc on query\n",
    "            if '*' in query:\n",
    "                completeTerms = getCompleteTerms()\n",
    "                pre = query.split(\" \")\n",
    "                for t in pre:\n",
    "                    if '*' in t:\n",
    "                        newStr = wildCard_bigram_handle(t,index)\n",
    "                        query = query.replace(t,newStr)\n",
    "            print(query)\n",
    "            postfixList = queryToPostfix(query)\n",
    "            \n",
    "            # I would rather manipulate on list of postings rathe then on words\n",
    "            for token in postfixList:\n",
    "                if token == \"OR\" or token == \"AND\" or token == \"AND_NOT\":\n",
    "                    totalPostings.append([token])\n",
    "                else:\n",
    "                    p1 = []\n",
    "                    print(token+'!!!')\n",
    "                    #atemming should be done here\n",
    "                    t = db.normalization(db.wordStemming([token]))[0]\n",
    "                    print(\"This is after: \"+t)\n",
    "                    if t in index:\n",
    "                        p1 = [d[0] for d in index[t]]\n",
    "                        totalPostings.append(p1)\n",
    "            print(\"Printing tootalpostings\")\n",
    "            print(totalPostings)\n",
    "            for token in totalPostings:\n",
    "                print(termsPostings)\n",
    "                if token[0] == \"OR\" or token[0] == \"AND\" or token[0] == \"AND_NOT\":\n",
    "                    p1 = termsPostings.pop()\n",
    "                    p2 = termsPostings.pop()\n",
    "                    termsPostings.append(findPostings(token[0],p1,p2))\n",
    "                else:\n",
    "                    termsPostings.append(token)\n",
    "            return termsPostings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompleteTerms():\n",
    "    with open('../output/terms.json', 'r') as file:\n",
    "        f = json.load(file)\n",
    "        temp = []\n",
    "        for l in f[\"terms\"]:\n",
    "            temp = list(set(temp+l))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printer AND ( laser OR ink )\n",
      "['printer', 'laser', 'ink', 'OR', 'AND']\n",
      "printer!!!\n",
      "This is after: printer\n",
      "laser!!!\n",
      "This is after: laser\n",
      "ink!!!\n",
      "This is after: ink\n",
      "Printing tootalpostings\n",
      "[[2], [2, 3, 5, 11, 12, 16, 17, 18, 20, 22, 23], [4, 5, 9, 10, 15, 19, 20], ['OR'], ['AND']]\n",
      "[]\n",
      "[[2]]\n",
      "[[2], [2, 3, 5, 11, 12, 16, 17, 18, 20, 22, 23]]\n",
      "[[2], [2, 3, 5, 11, 12, 16, 17, 18, 20, 22, 23], [4, 5, 9, 10, 15, 19, 20]]\n",
      "[[2], [2, 3, 4, 5, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"( *er OR ink )\"\n",
    "demo_processWithIndex(\"printer AND ( laser OR ink )\",[],index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {'zeroknowledg':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "         'printer':[[2,4]],\n",
    "         'laser':[[2,0],[3,0],[5,0],[11,0],[12,0],[16,0],[17,0],[18,0],[20,0],[22,0],[23,0]],\n",
    "         'ink': [[4,0],[5,0],[9,0],[10,0],[15,0],[19,0],[20,0]],\n",
    "         'printas':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "         'prin':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "         'pri':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "         'goetc':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "         'zxctc':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "         'asdasetc':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "         'zasdvctc':[[1,0],[3,0],[6,0],[11,0],[13,0],[16,0],[18,0],[19,0],[20,0]],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  printer  \n"
     ]
    }
   ],
   "source": [
    "temp = \" () \"\n",
    "for s in strl:\n",
    "    temp = temp[1]+\" \"+s+\" \"+temp[-2]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ter\n",
      "( a OR ink )\n"
     ]
    }
   ],
   "source": [
    "test = \"( *ter OR ink )\"\n",
    "if '*' in test:\n",
    "    pre = test.split(\" \")\n",
    "    for t in pre:\n",
    "        if '*' in t:\n",
    "            print(t)\n",
    "            newStr = \"a\"\n",
    "            test = test.replace(t,\"a\")\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toad\n",
      "asd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'toad'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in t.split('*'):\n",
    "    print(i)\n",
    "t.split('*')[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
