{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chenzihan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenzihan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams\n",
    "import dictionary_building_module as db\n",
    "import json\n",
    "from collections import defaultdict,Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpusPath = '../output/test.json'\n",
    "corpusPath = '../output/reuterStorage.json'\n",
    "allTerms = '../output/terms.json'\n",
    "# allTerms = '../output/testTerms.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bigram():\n",
    "    with open (corpusPath,'r') as f:\n",
    "        file = json.load(f)\n",
    "        pre_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        m = dict()\n",
    "        i = 0\n",
    "        f = open(allTerms,'r')\n",
    "        terms = json.load(f)\n",
    "       # term = Counter(terms['terms'][0])\n",
    "        counter = Counter()\n",
    "        for t in terms['terms']:\n",
    "            for word in t:\n",
    "                counter.update({word:1})\n",
    "        f.close()\n",
    "        for doc in file:\n",
    "            # remove stop words, nltk tokenize\n",
    "            sentence = db.stopWordRemoval(db.tokenize(doc['desc']))\n",
    "            for w1,w2 in bigrams(sentence,pad_right=True, pad_left=True):\n",
    "                if not w1 is None and not w2 is None:\n",
    "                    pre_model[w1][w2] +=1\n",
    "\n",
    "\n",
    "            for w1, w2 in pre_model.items(): # only keep words with min frequenct > x \n",
    "                for k,v in w2.items():\n",
    "                    if v > 4:\n",
    "                        model[w1][k] = v\n",
    "\n",
    "    #         f = open(allTerms,'r')\n",
    "    #         terms = json.load(f)\n",
    "    #         term = Counter(terms[\"0\"])\n",
    "            # P(w2|w1) = Count(w1w2)/Count(W2)\n",
    "            for w1 in model:\n",
    "                # total_count should be number of counts of W2 \n",
    "\n",
    "                for w2 in model[w1]:\n",
    "                    total_count = counter[w2]\n",
    "\n",
    "                    model[w1][w2] = model[w1][w2]/total_count \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callOnce():\n",
    "    model = bigram()\n",
    "    with open('../output/bigramModel.json','w') as f:\n",
    "        json.dump(model, f, sort_keys=True, indent=4,ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
