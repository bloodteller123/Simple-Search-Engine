{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,hamming_loss\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import dictionary_building_module as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataPath = '../output/test.json'\n",
    "dataPath = '../output/reuterStorage.json'\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "X_unknown = []\n",
    "all_topics = set()\n",
    "#https://stats.stackexchange.com/questions/233275/multilabel-classification-metrics-on-scikit\n",
    "#https://stackoverflow.com/questions/10715965/add-one-row-to-pandas-dataframe?answertab=active#tab-top\n",
    "#https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "#https://www.kaggle.com/roccoli/multi-label-classification-with-sklearn\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "#https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#https://stackoverflow.com/questions/48467669/tfidf-transformer-sklearn-results-in-no-supported-conversion-for-types-dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataPath, 'r') as f:\n",
    "    data = []\n",
    "    df = pd.DataFrame(columns=['docId', 'desc', 'topics'])\n",
    "    documents = json.load(f)\n",
    "    i=0\n",
    "    for doc in documents:\n",
    "        if (doc['topic'][0] != \"NO_TOPIC\") and (doc['desc'] != \"NO_CONTENT\"): # append new row to df\n",
    "            df.loc[i] = [doc['docId']] + [stringProcess(doc['desc'])]+[tuple(sorted(doc['topic']))]\n",
    "            i = i+1\n",
    "\n",
    "            \n",
    "        elif doc['topic'][0] == \"NO_TOPIC\" and doc['desc'] != \"NO_CONTENT\" :\n",
    "            X_unknown.append((doc['docId'], stringProcess(doc['desc'])))\n",
    "        #ignore file with not desc/no topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringProcess(desc):\n",
    "    return ' '.join(db.normalization(\n",
    "                    db.wordStemming(\n",
    "                        db.stopWordRemoval(\n",
    "                            db.tokenize(desc))))) # strings preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8666"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>desc</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>us econom data week could key determin whether...</td>\n",
       "      <td>(interest, ipi, retail)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>oper shr loss two ct vs profit three ct oper n...</td>\n",
       "      <td>(earn,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>shr 25 ct vs 36 ct net 14 mln vs 14 mln rev 56...</td>\n",
       "      <td>(earn,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>shr loss 102 dlr vs 101 dlr net loss 181 mln v...</td>\n",
       "      <td>(earn,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>usx corp said prove reserv oil natur ga liquid...</td>\n",
       "      <td>(crude, iron-steel, nat-gas)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docId                                               desc  \\\n",
       "0     5  us econom data week could key determin whether...   \n",
       "1    12  oper shr loss two ct vs profit three ct oper n...   \n",
       "2    14  shr 25 ct vs 36 ct net 14 mln vs 14 mln rev 56...   \n",
       "3    15  shr loss 102 dlr vs 101 dlr net loss 181 mln v...   \n",
       "4    16  usx corp said prove reserv oil natur ga liquid...   \n",
       "\n",
       "                         topics  \n",
       "0       (interest, ipi, retail)  \n",
       "1                       (earn,)  \n",
       "2                       (earn,)  \n",
       "3                       (earn,)  \n",
       "4  (crude, iron-steel, nat-gas)  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "y = multilabel_binarizer.fit_transform(df['topics'])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['desc'], y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors = 20,metric = 'minkowski', p=2)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss = 0.003937760075128317\n"
     ]
    }
   ],
   "source": [
    "predicts = classifier.predict(X_test)\n",
    "print(\"Hamming loss =\",hamming_loss(y_test,predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docId,des in X_unknown:\n",
    "    topics = classifier.predict(vectorizer.transform([des]))\n",
    "    topi = multilabel_binarizer.inverse_transform(topics)\n",
    "    top = topi[0]\n",
    "    temp = {\"topic\": top}\n",
    "    for d in documents:\n",
    "        if d['docId'] == docId:\n",
    "            d.update(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/new.json\", \"w\") as jsonFile: # to replace original corpus\n",
    "    json.dump(documents, jsonFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
